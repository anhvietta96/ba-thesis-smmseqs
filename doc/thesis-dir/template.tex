%% Example file for a masterthesis
% Copyright 2010 by Dirk Willrodt <dwillrodt@zbh.uni-hamburg.de>
% --------------------------------------------------------------
%
% This file may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.2
% of this license or (at your option) any later version.
% The latest version of this license is in:
%
%    http://www.latex-project.org/lppl.txt
%
% and version 1.2 or later is part of all distributions of LaTeX
% version 1999/12/01 or later.
%
% Usage: just compile it with pdflatex, and see how it looks, and what caused
% this. You can also take it and change blocks marked like this:
%%%%>
% ...
%%%%<
% to produce Your own thesis.
%
% check chapters in sub folders for examples of usage of different mark-ups


% use the option ger for thesis in german.
% use the option bsc for a bachelor thesis.
% you can pass options to scrreprt, for example numbers=noenddot for if you have
% an appendix and you don't want '.' at the end of chapter numbers
%\documentclass[twoside,a4paper,ger,bsc]{master}
\documentclass[twoside,a4paper,bsc]{master}
%%%%>
% the following package is only needed to produce dummy text.
% You do not need it for your thesis.
%\usepackage{lipsum}
% This package is not included in the class, because it is not needed, but we
% recommend to use it. It only works with pdfLaTeX.
\usepackage[kerning,spacing]{microtype}

% add other packages, as needed.
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{palatino}
\usepackage{numprint}
\usepackage{svg}
\usepackage{comment}
\usepackage{float}
\usepackage{times}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{amssymb}
\usepackage{dcolumn}

\usetikzlibrary{chains}
\usetikzlibrary{decorations.pathmorphing}
\usepgflibrary{decorations.pathreplacing}
\usetikzlibrary{decorations.text}

%%%Comment macro
\newcounter{mycommentcounter}
\newcommand{\Genericcomment}[2]{%
\par%
\noindent%
\fbox{%
\begin{minipage}{0.95\textwidth}
\textsl{#1: \#\refstepcounter{mycommentcounter}%
\arabic{mycommentcounter}: #2}%
\end{minipage}%
}%
\par%
}


\newcommand{\AVTcomment}[1]{
\Genericcomment{AVT}{#1}
}

\newcommand{\SKcomment}[1]{
\Genericcomment{SK}{#1}
}

%%% Other macros
\newcommand{\Qgram}[1]{\(#1\)-gram}
\newcommand{\Seed}[0]{\mathsf{seed}}
\newcommand{\InBlock}[0]{\mathsf{inBlock}}
\newcommand{\True}[0]{\mathsf{true}}
\newcommand{\False}[0]{\mathsf{false}}
\newcommand{\Rank}[0]{\mathsf{rank}}
\newcommand{\Blocks}[0]{\mathsf{blocks}}
\newcommand{\Block}[0]{\mathsf{block}}
\newcommand{\CharToAdd}[0]{\mathsf{CharToAdd}}
\newcommand{\PosToAdd}[0]{\mathsf{PosToAdd}}
\newcommand{\CharToRmv}[0]{\mathsf{CharToRmv}}
\newcommand{\PosToRmv}[0]{\mathsf{PosToRmv}}
\newcommand{\RankToAdd}[0]{\mathsf{RankToAdd}}
\newcommand{\RankToRmv}[0]{\mathsf{RankToRmv}}
\newcommand{\Query}[0]{\mathsf{Query}}
\newcommand{\Target}[0]{\mathsf{Target}}
\newcommand{\Append}[0]{\mathsf{append}}
\newcommand{\QueryIdx}[0]{\mathsf{QueryIdx}}
\newcommand{\TargetIdx}[0]{\mathsf{TargetIdx}}
\newcommand{\Start}[0]{\mathsf{start}}
\newcommand{\End}[0]{\mathsf{end}}
\newcommand{\TBlockEnd}[0]{\mathsf{TargetBlockEnd}}
\newcommand{\QBlockEnd}[0]{\mathsf{QueryBlockEnd}}
\newcommand{\HashValue}[0]{\mathsf{HashValue}}
\newcommand{\HashValVec}[0]{\mathsf{HashValVec}}
\newcommand{\CurrentWeight}[0]{\mathsf{CurrentWeight}}
\newcommand{\SuffixCode}[0]{\mathsf{SuffixCode}}
\newcommand{\Code}[0]{\mathsf{code}}
\newcommand{\Score}[0]{\mathsf{score}}
\newcommand{\Loopid}[0]{\mathsf{loopid}}
\newcommand{\Wheel}[0]{\mathsf{wheel}}
\newcommand{\IT}[2]{\mathsf{IndexTable}_{#1,#2}}
\newcommand{\LE}[2]{\mathsf{LinearEnc}_{#1,#2}}
\newcommand{\Substring}[3]{#1\lbrack #2\ldots#3\rbrack}
\newcommand{\Subchar}[2]{#1\lbrack #2\rbrack}
\newcommand{\Scoretablename}[0]{\mathsf{ST}}
\newcommand{\Scoretable}[2]{\Scoretablename_{#1}[#2]}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Permname}[1]{\varphi_{#1}}
\newcommand{\Perm}[2]{\Permname{#1}(#2)}
\newcommand{\Permnameinverse}[1]{\varphi_{#1}^{-1}}
\newcommand{\Perminverse}[2]{\Permnameinverse{#1}(#2)}
\newcommand{\Alpha}[0]{\mathcal{A}}
\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Qsplit}{\mathsf{QS}}

\theorembodyfont{\rmfamily}
\theoremheaderfont{\rmfamily\bfseries}
\newcounter{Lemmacounter}
\newtheorem{myLemma}[Lemmacounter]{Lemma}
\newcommand{\AfterNumber}{}
\newcommand{\Skiptheorem}{\smallskipamount}
\newcommand{\StartFormal}[1]{\par\addvspace{\Skiptheorem}\noindent\textbf{#1}}
\newcommand{\EndFormal}{\par\addvspace{\Skiptheorem}}
%\newenvironment{Lemma}{\begin{myLemma}\AfterNumber}{\end{myLemma}}
\newenvironment{Proof}{\StartFormal{Proof:}}{\EndFormal}
\def\FirstMultisetSymbol{\(a_0\)}
\def\SecondMultisetSymbol{\(a_1\)}
\def\ThirdMultisetSymbol{\(a_2\)}

%%%%<

% Helpfull command to put reminders in your work
\newcommand{\Todo}[1]%
{\colorbox{orange}{\mbox{}\\\fbox{\parbox{\textwidth}{\textbf{Todo: #1}}}}\\}
% it uses one argument which is referred to by #1

\begin{document}

% use roman page numbers before the real document starts.
\pagenumbering{roman}

%Change the values here to get your title page and assertion page right
\AdvisorA{Stefan Kurtz}
\AdvisorB{Andrew Torda}
\author{Anh Viet Ta}
\MatrikelNo{\numprint{6747004}}
%\AuthorAdress{Musterstrasse 1}{07770 M\"unchhausen}
\title{
       \begin{tabular}{c}
       \textbf{Efficient Preprocessing of Short Sequence Segments}\\[3mm]
       \textbf{for Efficient Filtering in Comparing Protein Sequences}
       \end{tabular}}
% add data line or leave it to use current date.
%\date{1.4.2016}
\Maketitle

\SKcomment{Please provide a longer abstract}

\SKcomment{There are again some unresolved reference.
I am tired to always remind you that you need to resolve these before pushing.}

% if you write in German, maybe change this to "Zusammenfassung".
\addchap*{Abstract}
\small
%%%%>
In analysis of protein and DNA sequences, the concept of homology is
central. It
may suggest that organisms share a common evolutionary ancestor when two
sequences or structures share more similarity than expected by chance. But
in
reality, homologous sequences do not always share significant sequence
similarity but instead their resemblances are based on statistically
significant
structure likeness or a connection to a intermediate sequence. There rises
a
need for algorithms not only to find exact matches in sequences, but also
to
find local similarities (sensitive searching) and cluster them into groups
(sequence clustering).
The software suite MMseqs2 and its predecessor MMseqs are the new standard
of both sensitive biological sequence searching and sequence clustering.
The prefiltering module in MMseqs2 is of particular importance, since it
serves to rapidly reduce possibility space. This thesis focusses on the
first phase of the filtering approach, i.e.\ the construction of the
\(k\)-environment and shows how to systematically improve its runtime and
space requirement.
%%%%<
\normalsize
% The first sets the depth of the TOC, the second produces it. Change the
% number if you need a ``deeper'' TOC.
\setcounter{tocdepth}{1}
\tableofcontents
%%%%>
% include chapters from subfolders
% change if you need more chapters or if you want to call them different,
this
% is just an example
\Chapter{Introduction}
\section{Recent developments of bioinformatic sequence searching and
clustering}
% reset to arabic page numbers will start with 1
\pagenumbering{arabic}
With the advent of high-throughput sequencing technologies, the cost of
sequencing DNA has dramatically be reduced. Sequence databases, such as
UniProt have been growing by a factor of two every two years , which leads
to a significant focus on developing searching and clustering methods that
can handle large-scale datasets efficiently. Algorithms and software that
can scale horizontally (across multiple machines) have gained importance.
On sequence searching, new algorithms and heuristics have been developed to
improve sensitivity without sacrificing speed. Tools like
DIAMOND~\cite{buchfink2015fast,BUC:REU:DRO:2021} and
MMseqs2~\cite{steinegger2017mmseqs2} provide fast and sensitive sequence
searching capabilities,
especially in metagenomics and large-scale sequencing projects.

Hidden
Markov Model based
methods, such as the HMMER~\cite{finn2011hmmer} software, have been
enhanced to improve their
sensitivity and accuracy, making them invaluable for protein domain and
family searching. The existing algorithms like BLAST have also seen massive
improvement by using GPU parallelization~\cite{vouzis2011gpu} and specific
hardwares, namely
FPGAs (Field-Programmable Gate Arrays)~\cite{guo2012systolic} have been
customized for
accelerating sequence searching algorithms.
Recent developments on metagenomics have also given ways to specialized
databases and algorithms. These databases contain sequences from
environmental samples and enable the identification of novel organisms and
genes within complex microbial communities.
On sequence clustering, myriads of methods have been developed. Graph-based
clustering methods, such as Markov Clustering
(MCL)~\cite{shih2012identifying} and the
Louvain~\cite{rahiminejad2019topological} algorithm,
have gained popularity. These methods model sequences as nodes in a graph
and use edge weights to represent similarities, allowing for the detection
of densely connected clusters within the graph. Density-based methods like
DBSCAN~\cite{ester1996density} (Density-Based Spatial Clustering of
Applications with Noise) have
seen applications in bioinformatics~\cite{karim2021deep}. These algorithms
group sequences based on
the density of data points, enabling the discovery of clusters with varying
shapes and sizes. Traditional distance-based clustering algorithms, such as
hierarchical clustering~\cite{lafond2017new} and
k-means~\cite{hussain2011fpga}, have been adapted and optimized for
large biological datasets.
\Chapter{MMseqs2}
MMseqs~\cite{hauser2014mmseqs} (Many-against-Many sequence searching) is a
software suite for fast
and deep clustering and searching of large datasets.
It consists of three core modules:
\begin{itemize}
\item a fast and sensitive preﬁltering module
that eliminates most non-homologous sequences,
\item a local alignment module based on the Smith-Waterman algorithm
using the ideas and implementation techniques described
in~\cite{Farrar:2007hs,ZHA:LEE:GAR:MAR:2013},
\item and a clustering module based on a
similary graph.
\end{itemize}
Due to the modularity of the structure, it allows users to
create workflows tailored to specific clustering and searching needs. The
default workflow utilizes the UniProt databases to create predefined
parameters and resembles the average use case. The second, cascaded
clustering, clusters the input database in multiple steps, increasing the
sensitivity between each steps and therefore enables finding
best-hit-searches 4-10 times faster compared to default workflow.
The third workflow simply compares
databases using only prefiltering and aligning modules. The last workflow
updates clustering between new and old clustered database by deleting
deprecated sequences and appending new sequences to each cluster.
In benchmarks, MMseqs showed to be 4-30 times faster than
UBLAST~\cite{edgar2010search} and RAPsearch2~\cite{zhao2012rapsearch2}.
MMseqs could cluster large databases down to 30\% sequence
identity \numprint{2000} times faster than BLASTclust.
An improved, updated version MMseqs2~\cite{steinegger2017mmseqs2} was
published in 2017.
\SKcomment{Do not use \textbackslash{numprint} for years.}
It provides dramatic improvements in both efficiency and sensitivity by
introducing several novel ideas:
\begin{itemize}
\item The prefiltering stage was revamped by
introducing an algorithm to find two consecutive, spaced-seed \Qgram{q}
matches
and optimizing memory access.
\item Extension to allow the usage of \Qgram{7} due to
the speedup of the algorithm.
\item Integration of MPI (Message-Passing-Interface) allows the
distribution of jobs on computer clusters.
\item Iterative profile search increases sensitivity far greater
than sensitivity of BLAST.
\end{itemize}
An in-depth look into the three core modules is presented below.
\SKcomment{I will skip reading the next section.}
\section{Fast \Qgram{q} prefiltering stage}
In MMseqs, the prefiltering stage is the first step to process all
sequences and serves to reduce the search space significantly, therefore
it's imperative for the algorithm to be as efficient as possible. The
process consists of 4 loops as outlined in
Figure~\ref{fig:prefilterMMseqs}. In the first loop, query sequences are
searched one by one against the target set. For each spaced seed window in
the query sequence (loop 2), a \Qgram{q} is extracted and a list of all
similar \Qgram{q}s with a Blosum62 similarity above a threshold score is
created using a linear-time branch-and-bound algorithm. Lower threshold
scores result in higher average numbers of similar k-mers and thereby
higher sensitivity and lower speed. Each generated \Qgram{q} (loop 3) then
gets queried against a precomputed database of \Qgram{q}s in target
sequences. The sum of scores of the corresponding target sequences then
gets collected in a vector based on the prefiltering score. In the last
step the target score vector is sorted and the best matches get processed
further.
\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.3]{graphics/MMseqs_prefilter.png}
\end{center}
\caption{Prefiltering stage in MMseqs~\cite{hauser2014mmseqs}}
\label{fig:prefilterMMseqs}
\end{figure}
In MMseqs2, the prefiltering stage was reengineered (See
Figure~\ref{fig:prefilterMMseqs2}). After the query-target matches are
collected, they get sorted and iterated over to detect double k-mer matches
by comparing the current diagonal with the previously matched diagonal. If
the previous and current diagonals agree, they both get stored as a double
match for the ungapped alignment stage.
\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.7]{graphics/MMseqs2_prefilter.png}
\end{center}
\caption{Prefiltering stage in MMseqs2~\cite{steinegger2017mmseqs2}}
\label{fig:prefilterMMseqs2}
\end{figure}
\section{Ungapped alignment and Smith-Waterman alignment stage}
In MMseqs2, an additional ungapped alignment stage was introduced, where
many target sequences are aligned at once using a vectorized approach. It
also utilizes a linear memory access using a score matrix containing the p.
Due to the extensive use of SIMD instructions, this stage achieves a linear
time complexity, much more efficient compared to the Smith-Waterman
alignment stage.
After the ungapped alignment is completed, MMseqs2 computes for all
filtered
query-target sequence pairs an exact, unbounded Smith-Waterman alignments
with affine gap penalties. This module utilizes a highly vectorized
algorithm
equipped with new SIMD instructions and adapted for sequence profiles.
\Chapter{Methods}
\SKcomment{This chapter should be named Methods.}
\AVTcomment{Fixed.}
For every alphabet \(\Alpha\), we can define a bijective mapping to a
subset
of \(\Nats\) of size \(|\Alpha|\). Therefore, in the
following sections we consider sequences over \(\Alpha\) as sequences of
integers, where each integer encodes a different character of \(\Alpha\).
That is, the alphabet will be interpreted mostly as an integer
subset \([0,|\mathcal{A}|)\cap \Nats\).
\section{Optimizing target data processing}

\SKcomment{Actually the use of spaced seeds is not restricted to the target
data base. You also extract the \(q\)-grams from the query sequence and from these you generate the \(k\)-environment. This should be explained. As a
consequence, the header of this section should be more general, like
Efficient sequence encoding for spaced  seeds}
Since the target database can be very large, a precise and efficient
evaluation must be prioritized. In MMseqs2, at every residue position,
relevant characters based on the user-chosen spaced seed are individually
extracted from the target sequence to create \Qgram{q}s and then process
these \Qgram{q}s as independent strings. This approach requires to compute
a hash value for each \Qgram{q} using \(O(q)\) time.

\SKcomment{Where was \(q\) introduced? It should be the weight of the
spaced seeds. This should be made explicit.}

For a sequence of
length
\(n\), this would sum up to \(O((n-q+1)q)=O(nq)\). This process is detailed
in
Algorithm~\ref{mmseqshash}.
\begin{algorithm}[t]
\caption{Invertible Integer Hashing using spaced seeds}
\label{code:mmseqshash}
\begin{tabular}{@{}l@{~}l}
\textbf{Input:}
&spaced seed \(\Seed\)\\
&Encoded sequence \(s\) of length \(n\)\\
&radix \(r = |\Alpha|\)
\end{tabular}
\begin{algorithmic}
\State \(\HashValVec\gets []\)
\State \(\HashValue \gets 0\)
\For{\(j\) \textbf{from} 0 to $n-k$}\Comment{Iterate over sequence}
\For{\(i \in [0,\Seed.\mathsf{span})\)}\Comment{Iterate over seed}
\If{\(\Seed[i]\)}
\State \(\HashValue\text{ = }\HashValue\cdot r\text{ + }s[i]\)
\EndIf
\EndFor
\State \(\HashValVec .\Append(\HashValue)\)
\EndFor
\State return \(\HashValVec\)
\end{algorithmic}
\end{algorithm}
In order to optimize this process, a recursive hashing
algorithm~\cite{cohen1997recursive} could be implemented, where the spaced
seeds
are considered as a shifting hash window. To gather the next hash value,
the
window can be shifted by one, resulting in the removal and addition of old
and
new characters respectively, which are both determined by the spaced seed.
In
fact, the old characters can be characterized as the start of a block of
consecutive \Qgram{q}s, and the new character respectively as the
character
directly after the end of the block. This schema is visualized in
Figure~\ref{fig:recursive}.

\SKcomment{I assume that you have defined a block as a maximum sequence of
1-bits of a spaced seed. But then the new character directly after the end of the block would correspond to a 0 bit in the spaced seed, i.e.\ it is irrelevant
for the. Deducing this, This does not make sense.}
\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{graphics/recursive.png}
\end{center}
\caption{a) Schema of invertible integer hashing and b) decomposition of
spaced
seed into blocks of consecutive \Qgram{q}}
\label{fig:recursive}
\end{figure}

\SKcomment{You need to give more explanation for Figure 3.1. Also the figure
suggests that the prefix of length \(q-1\) corresponds to the initial
block 1-bits. But \(q\) is the total number of 1-bits in the spaced seed.
As a consequence, the figure is misleading.}

\SKcomment{But this previous statement only holds for consecutive
\(q\)-grams, not for those \(q\)-grams extracted by using spaced-seeds.
But the context suggests that you are referring to the latter. So please
be more precise.}
\SKcomment{Unclear what you by this 'family of hash functions'? You likely
mean incremental/recursive hash functions?}
\AVTcomment{Fixed.}
\SKcomment{Please explain how you have fixed this.}
Assuming the removal and addition operations on characters are of constant
time
complexity, the algorithm would run in \(O(nb)\) time, where \(b\) is the
number
of consecutive \Qgram{q} blocks. The simplest case of the algorithm, where
the
spaced seed itself is a consecutive \Qgram{q} (\(b=1\)), is called
invertible
integer hashing~\ref{gsa}, and it employs two fundamental equations.
In order to achieve independence between hash values, the characters are
usually
assigned a weight, which might be expressed as \(r^{q-1-i}\), where \(r\)
is
called the radix
and \(i\) is the position of the character in the
\Qgram{q}.
Since the weight of every character must be unique in every position in the
\Qgram{q}, the radix must be large enough to discern every character in
the alphabet and usually assigned as the alphabet size itself
(\(r=|\Alpha|\)).
The calculation of a hash value from a \Qgram{q} \(w\) could then be
expressed
as follows:
\begin{align}
H(w) = \sum_{i=0}^{q-1}r^{q-1-i}w[i]\label{DefineHfunction}
\end{align}
where $w$ is a \Qgram{q} and \(w[i]\) is the \(i\)-th character in \(w\).
As a base case of the recursive algorithm, the hash value for the
first \Qgram{q} of the sequence is computed in \(O(k)\) time by evaluating
the sum defined in Equation (\ref{DefineHfunction}).
Provided we have calculated the hash value of the previous \Qgram{q}
\(ax\), where \(a\) is a character and \(x\) is a \Qgram{q-1}.
Then the hash value
of the next \Qgram{q} could be computed from \(H(ax)\) and the next not yet
processed character, say \(c\). Firstly, the contribution of \(a\)
character is subtracted from the hash value. The virtual window is then
shifted right one character. This means that the weight of all characters
in \(x\) increases by a factor or \(r\). That is, we multiply by the radix.
Lastly, the new character \(c\) is added to the hash.
The hash value of \(xc\) is then calculated as follows:
\begin{align}
H(xc) = (H(ax)-r^{n-1}\cdot a)\cdot r+c\label{IncrementallyComputeH}
\end{align}
The pseudocode for this schema is outlined in Algorithm \ref{code:invint}.
\begin{algorithm}[t]
\caption{Invertible Integer Encoding}
\label{code:invint}
\begin{tabular}{@{}l@{~}l}
\textbf{Input:}&Encoded sequence $s$ of length $n$\\
&alphabet size \(r=|\mathcal{A}|\)\\
\end{tabular}
\begin{algorithmic}
\State \(\HashValVec\gets []\)
\State \(\HashValue \gets 0\)
\For{\(i\) \textbf{from} 1 to $k$}\Comment{Compute 1st hash}
\State \(\HashValue \gets \HashValue \cdot r\)
\State \(\HashValue \gets \HashValue + s[i]\)
\EndFor
\State \(\HashValVec .\Append(\HashValue)\)
\For{\(j\) \textbf{from} 1 to $n-k$}\Comment{Compute other hash values}
\State \(\HashValue \gets \HashValue - r^{n-1}\cdot s[j]\)
\State \(\HashValue \gets \HashValue \cdot r\)
\State \(\HashValue \gets \HashValue + s[j+k]\)\
\State \(\HashValVec .\Append(\HashValue)\)
\EndFor
\State return \(\HashValVec\)
\end{algorithmic}
\end{algorithm}

In order to extend the schema to allow for spaced seeds, one could divide
the seed into shorter blocks of \Qgram{q}s, intertwined by blocks of

\SKcomment{Please do not use the notion of \(q\)-grams here, as, in general, the
blocks are different length and only the sum of the length of the blocks is
\(q\) (the weight) I suppose.}

"Don't Care". Each block can then be treated as individual \Qgram{q} with
custom radix, and the complexity is then \(O(nb)\), where \(b\) is the
number of blocks. The division of a seed into blocks is computed by
Algorithm~\ref{code:hashblock}.
\begin{algorithm}[t]
\caption{Divide spaced seed into blocks of consecutive \Qgram{q}}
\label{code:hashblock}
\begin{tabular}{@{}l@{~}l}
\textbf{Input:}&Spaced seed as bit vector \(\Seed\)\\
\end{tabular}
\begin{algorithmic}
\State \(\InBlock \gets \False\)
\State \(\Start \gets 0\)
\State \(\Blocks \gets []\)
\State \(\Rank \gets \Seed.\mathsf{weight} - 1\)
\For{\(i \in [0,\Seed.\mathsf{span})\)}\Comment{Iterate over seed}
\If{\(\Seed[i] \text{ and } \text{not }\InBlock\)}\Comment{Start new block}
\State \(\InBlock \gets \True\)
\State \(Start \gets i\)
\Else
\If{\(\text{not }\Seed[i] \text{ and } \InBlock\)} \Comment{End current block}
\State \(\InBlock \gets \False\)
\State \(\Blocks.\mathsf{append}(\{\begin{tabular}{c}
\(\CharToRmv:\Start\),\\
\(\RankToRmv:r^{\Rank+i-1-\Start}\),\\
\(\CharToAdd:i\),\\
\(\RankToAdd:r^{\Rank}\)\end{tabular}\})\)
\EndIf
\EndIf
\If{\(\Seed[i]\)}
\State \(\Rank-=1\)
\EndIf
\EndFor
\If{\(\InBlock\)}
\State \(\Blocks.\mathsf{append}(\left(\begin{tabular}{c}
\(\CharToRmv:\Start\),\\
\(\RankToRmv:r^{\Rank+\Seed.\mathsf{span}-1-\Start}\),\\
\(\CharToAdd:\Seed.\mathsf{span}\),\\
\(\RankToAdd:r^\Rank\)\end{tabular}\right))\)
\EndIf
\State return \(\Blocks\)
\end{algorithmic}
\end{algorithm}
The output of the dividing algorithm is a vector of blocks, represented
internally as quartet of character to remove, rank to remove, character to
add
and rank to add.

\SKcomment{The notation reminds of a dictionary, but you have not explained
your notation and the keys of the dictionary, which are abbreviations of what
you just stated. Actually, \(\CharToRmv\) and \(\CharToAdd\) do not denote
characters, but the positions of character to remove/add and you actually
use these as positions. So use \(\PosToRmv\) and \(\PosToAdd\).
The other values are simply
powers of \(r\) depending in the current rank value.
Also in the case after the for-loop, rank should be 0, as all 1-bits have been
processed. This allows to simplify the quartet value. Note also the bit
brackets for the second 4-tuple. This should be used for the first 4-tuple too.}

\SKcomment{When using operator like assign and add plese typeset them as text, like in \(\text{ += }\)}.

This preprocessing of spaced seeds runs in linear time
\(O(\Seed.\mathsf{span})\), same as the computation of the first hash.
After the blocks are created, the recursive hashing step, detailed in
Algorithm~\ref{invintseed} can proceed. Here the blocks are firstly
iterated one
by one to subtract the hash component of every character to be removed. The
hashing window can then be shifted by 1 character, same as multiplying with
the
radix \(|\Alpha|\). Then the blocks are iterated once again to add the hash
component of new characters.

\SKcomment{It is not clear to me, why the rank values you computed are
correct. Please add an explanation (which I do not see).}
\begin{algorithm}[t]
\caption{Invertible Integer Hashing using spaced seeds}
\label{code:invintseed}
\begin{tabular}{@{}l@{~}l}
\textbf{Input:}
&spaced seed \(\Seed\)\\
&Blocks of consecutive \Qgram{q}s in spaced seed \(\Blocks\)\\
&Encoded sequence \(s\) of length \(n\)\\
&radix \(r = |\Alpha|\)
\end{tabular}
\begin{algorithmic}
\State \(\HashValVec\gets []\)
\State \(\HashValue \gets 0\)
\For{\(i \in [0,\Seed.\mathsf{span})\)}\Comment{Iterate over seed}
\If{\(\Seed[i]\)}
\State \(\HashValue *= r\)
\State \(\HashValue += s[i]\)
\EndIf
\EndFor
\State \(\HashValVec .\Append(\HashValue)\)
\For{\(j\) \textbf{from} 1 to $n-k$}\Comment{Compute other hash values}
\For{\(\Block \in \Blocks\)}
\State \(\HashValue -= \Block.\RankToRmv \cdot s[\Block.\CharToRmv + j]\)
\EndFor
\State \(\HashValue *= r\)
\For{\(\Block \in \Blocks\)}
\State \(\HashValue += \Block.\RankToAdd \cdot s[\Block.\CharToAdd + j]\)
\EndFor
\State \(\HashValVec.\Append(\HashValue)\)
\EndFor
\State return \(\HashValVec\)
\end{algorithmic}
\end{algorithm}

\SKcomment{The algorithm uses a value \(k\) which is not defined.
You likely means the span of the seed.}

In Algorithm \ref{code:invintseed},
for each block, two operations
must
be performed, a subtraction and an addition, which means that the expected
run
time of the algorithm should be proportional to \(2nb\).

\SKcomment{But you also have an addition of positions, a table lookup
and a multiplication. All these occur twice. Furthermore there is one
multiplication. So the number of operation is \(8nb\). Now determine the
number \(X\) of operations per 1-bit in the space seed. There are
\(nXq\) operations you need. So you can no calculate the factor
\(Xq/8b\). For an asymptotic consideration you can ignore X and 8, but for
calculating the speedup they are relevant.}

Compared to the method
of direct extraction and hashing in MMseqs2, which scales linearly with
\(nq\),
this approach presents in the worst case of \(q=b\) double the run time.
The runtime is then expected to be equal when \(q=2b\) and for all
\(b < \frac{q}{2},\)
recursive hashing is expected to present a better run time than the current
hashing approach. In Table~\ref{tab:seeds}, the default seeds used in
MMseqs2
are listed, along with their theoretical hashing speedup using recursive
hashing.
\begin{table}
\begin{center}
\begin{tabular}{c|c|r|c|c|c}
No & Seed & Span & Weight \(q\) & Number of & Speedup \\
   &      &      &              & Blocks \(b\) & factor \\
\hline
1& \numprint{11101} & 5 & 4 & 2 & 1.000 \\
2& \numprint{111011} & 6 & 5 & 2 & 1.250\\
3& \numprint{1101011} & 7 & 5 & 3 & 0.830 \\
4& \numprint{110010000101} & 12 & 5 & 4 & 0.625\\
5& \numprint{11101101} & 8 & 6 & 3 & 1.000\\
6& \numprint{1101010011} & 10 & 6 & 4 & 0.750\\
7& \numprint{1111010101} & 10 & 7 & 5 & 0.700\\
8& \numprint{11010110011} & 11 & 7 & 4 & 0.875\\
\end{tabular}
\caption{Seeds used in MMseqs2 with their span, weight, number of blocks
and theoretical hashing speedup factor $\frac{q}{2b}$
using recursive hashing\label{tab:seeds}}
\end{center}
\end{table}
\SKcomment{You need to be more precise here. So the existing methods
involving spaced seeds need \(O(nq)\) time, while your block based method
needs
\(O(nb)\) time, where \(b\leq q\). So your method reduces the runtime by
a factor of \(\frac{q}{b}\). If \(b=q\), there would be no speedup. The
case
\(b=1\) would be the case of consecutive \(q\)-grams with a speedup
of \(O(q)\), i.e.\ result in a runtime of \(O(n)\). So your method would
generalize the method explained above.}
\SKcomment{Do you have a table with the spaced seeds, including the number
of
blocks. This would directly allow to calculate the speedup for each
spaced-seed.}
\section{Optimizing query data processing}
In order to quantify the similarity between sequences, one uses a scoring
function. This provides quantitative measures for comparing biological
sequences, structures, and interactions, facilitating a deeper
understanding of biological processes and aiding in drug discovery,
evolutionary analysis, and functional genomics research.
\SKcomment{Omit the part beginning with structures. This is general bla
bla,
not relevant here.}
In sequence
alignment context, these functions often compare pairwise the biological
residues (often nucleotide bases or proteinogenic amino acids). The most
common scoring function is the substitution matrix, often represented as a
matrix of scores indicating the likelihood of one amino acid (or
nucleotide) being replaced by another. The BLOSUM (Blocks Substitution
Matrix) and PAM (Point Accepted Mutation) matrices are examples of
substitution matrices widely used in bioinformatics for quantifying
similarity
between amino acid sequences.
\SKcomment{You need to mention that BLOSUM and PAM are for amino acids.}
The two families of
matrices can be summarized as a mathematical function:
\begin{align}
\sigma: \mathcal{A} \times \mathcal{A} \rightarrow \Reals
\end{align}
A positive score between two residues often denotes similarity between them
and a negative score indicates dissimilarity. The score between two
sequences \(u,v\in \Alpha^q\) can then be defined as follow:
\begin{align}
\sigma(u,v)=\sum_{i=0}^{q-1}\sigma(\Subchar{u}{i},\Subchar{v}{i})\label{equation:sequenceScore}
\end{align}
The \(k\)-environment of a \Qgram{q} \(u\) is then defined as in~\cite{gsa}:
\begin{align}
\text{Env}_{k,\sigma}[u] = \{(v,\sigma(u,v))\mid v\in \Alpha^q,\sigma(u,v)\geq
k\}
\end{align}
In order to enumerate the \(k\)-environment, the most direct
method is to calculate a score table
\begin{align}
\Scoretable{q}{u}=\lbrack (v,\sigma(u,v))\mid v\in\Alpha^{q}\rbrack
\end{align}
in which the pairs are sorted in descending order of the the score-value.
The \(k\)-enviroment
can then be extracted as the initial part of \(\Scoretable{q}{u}\) up to the
last pair whose score is \(\geq k\).
This approach is often infeasible for large \(q\) due to fact that there
are \(O(|\Alpha|^{q})\) \(q\)-grams. An alternative
approach
is a branch-and-bound algorithm~\cite{land2010automatic}, where the
\Qgram{q} is
decomposed into smaller sub-\Qgram{q}. In its most simplistic form, the
algorithm constructs a score table \(\Scoretable{1}{a}\) for all \(a\in\Alpha\).
Then the

\SKcomment{Note that you have used indexing from 1 in other context.
Always use the same indexing. I would prefer from 0.}

\(k\)-environment is contructed as a subset of the cartesian product
\(\bigtimes_{i=0}^{q-1} \Scoretable{1}{\Subchar{u}{i}}\):
\begin{align}
\text{Env}_k(u) = \{(v_{0}v_{1}\ldots v_{q-1},s) \mid
& ~((v_{0},s_{0}),(v_{1},s_{1}),\ldots (v_{q-1},s_{q-1}))\in\bigtimes\nolimits_{i=0}^{q-1}\Scoretable{1}{\Subchar{u}{i}},\nonumber\\
& ~s:=\sum\nolimits_{i=0}^{q-1}s_{i}\geq k\}
\label{eq:singleCharCartesian}
\end{align}

Here \(s:=\) means assignment to the local variable \(s\), to simplify the
expression.

The approach presented in MMseqs2 utilizes both approaches, where instead of

\SKcomment{What is the second approach you refer to?}

single characters, it decomposes \(u\) into groups of \Qgram{2}s and
\Qgram{3}s using a splitting function \(\Qsplit\). For every
integer \(q\), the function delivers a list \(\Qsplit(q)\)  such that

\begin{itemize}
\item the output is a list of length
\(\left\lfloor\frac{q}{3}\right\rfloor+(1\text{ if } (q\text{ mod } 3 \neq 0)\text{ else }0)\),
\item \(q'\in\{2,3\}\) for all \(q'\in\Qsplit(q)\),
\item \(q = \sum_{q'\in\Qsplit(q)}q'\),
\item \(\Qsplit(q)\) is sorted in descending order.
\end{itemize}
Table~\ref{tab:splitting} shows \(\Qsplit(q)\) for
\(q\in\{4,5,6,7\}\).

\begin{table}
\begin{center}
\begin{tabular}{c|l}
\(q\) & \(\Qsplit(q)\)\\
\hline
\(4\) & \([2,2]\)\\
\(5\) & \([3,2]\)\\
\(6\) & \([3,3]\)\\
\(7\) & \([3,2,2]\)\\
\end{tabular}
\caption{Examples of application of splitting function \(\Qsplit\) on different
\Qgram{q} length}\label{tab:splitting}
\end{center}
\end{table}

For any \(q\)-gram \(u\),
let \(\Qsplit(q,u)\) denote the splitting of \(u\) into non-overlapping
substrings \(u_{i}\) of length according to \(\Qsplit(q)\).

For each of these substrings, a score table \(\Scoretable{|u_{i}|}{u_i}\) is
precomputed and the \(k\)-environment can then be enumerated similarly to
Equation~\ref{eq:singleCharCartesian}

\SKcomment{I had corrected Equation~\ref{eq:singleCharCartesian}. Please
apply analoguous corrections to the following equation.}

\begin{align}
\text{Env}_k(u) = \{(v,\sigma(u,v))\mid (v,\sigma(u,v))\in \bigtimes
\Scoretable{|u_i|}{u_i},\sigma(u,v)\geq k\}\label{ScoreTablesCartesian}
\end{align}

The iteration over score matrix row can be prematurely terminated, as soon
as it is no longer feasible to reach the threshold \(k\).

The key-idea to improve the construction of the \(k\)-environment
is to reorder the characters of the \(q\)-grams. Let
\(\varphi:\{0,1,\ldots,q-1\}\to\{0,1,\ldots,q-1\}\) be a bijective mapping
and define
\begin{align}
\varphi(u)=\Subchar{u}{\varphi(0)}\Subchar{u}{\varphi(1)}\ldots\Subchar{u}{\varphi(q-1)}
\end{align}
for any \(u\in\Alpha^{q}\). That is, \(\varphi\) permutes the
characters in \(u\). As it permutes \(q\) characters, we call it a
\(q\)-permutation. The following lemma
states that the score for a pair of \(q\)-grams does not change if both are
permuted in the same way.
\begin{Lemma}
\label{permutationProof}
For all \(q\)-permutations \(\varphi\) and all \(u,v\in\Alpha^{q}\) it
holds:
\begin{align}
\sigma(\varphi(u),\varphi(v))=\sigma(u,v)
\end{align}
\begin{Proof}
\begin{align}
\sigma(\varphi(u),\varphi(v))
&= \sum_{j=0}^{q-1} \sigma (\Subchar{\varphi(u)}{j},
\Subchar{\varphi(v)}{j})\\
&= \sum_{j=0}^{q-1} \sigma
(\Subchar{u}{\varphi(j)},\Subchar{v}{\varphi(j)})\label{eq:jPerm}\\
&= \sum_{i=0}^{q-1} \sigma
(\Subchar{u}{i},\Subchar{v}{i})\label{eq:iPerm}\\
&= \sigma(u,v)
\end{align}
The expressions in (\ref{eq:jPerm}) and (\ref{eq:iPerm}) are equal,
as \(\sigma\) is applied to the same pairs of characters in
a possibly different order.
\end{Proof}
\end{Lemma}
An example of this scoring characteristic under permutation is illustrated
in
Figure~\ref{shuffle}. The premutation function is internally represented as
an
array \(p\) of the same length as the seed weight. Upon applying the
permutation
function, the \(i\)-th character gets shuffled to \(\Subchar{p}{i}\).
\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{graphics/shuffle.png}
\end{center}
\caption{Example of a permutation function and a graphical presentation of
the
sequence score invariance under permutation on both sequences.}
\label{fig:shuffle}
\end{figure}
\AVTcomment{The notation here is slightly different. I'll try to redo so
that
they are similar to text. Placeholder only.}
The sorting \(q\)-permutation \(\varphi_{u}\) is of special interest.
It sorts the characters in \(u\), i.e.\ it satisfies
\begin{align}
\Subchar{u}{\Perm{u}{i}}\leq\Subchar{u}{\Perm{u}{i+1}}\text{ for all }i,
0\leq
i\leq q-2
\end{align}
The resulting \Qgram{q} \(u_s=\varphi_{u}(u)\) is called sorted \Qgram{q}.
The inverse of \(\varphi_{u}\) is \(\varphi_{u}^{-1}\), defined by
\begin{align}
\Perminverse{u}{\Perm{u}{i}}=i\text{ for all }i, 0\leq i\leq q-1
\end{align}
As a consequence \(\Perminverse{u}{\Perm{u}{u}}=u\).
From the Lemma we can deduce, that it suffices to precompute the
the score tables \(\mathsf{ST}_{q'}\) for \(q'\in\{2,3\}\)
only for the sorted \(q'\)-grams. From these one can compute
the \(k\)-environment of a sorted \(q\)-gram whose members are finally
transformed by the inverse of the sorting permutation.
The advantage of this approach is the smaller number of sorted \Qgram{q}s
compared to unsorted, leading to a more efficient memory usage and a more
compacted computation of the score matrix. Table~\ref{tab:numdiff} outlines
the
difference in number of \Qgram{q}s to be enumerated and the size of the
resulted
score matrices.

\SKcomment{Please add a column with the ratio of the shown numbers in table
\ref{tab:numdiff}.}

\begin{table}
tab:numdiff\begin{center}
\begin{tabular}{c|c|r|r}
\Qgram{q} type & \(q\) & Number & Score matrix \\
               &       & of \Qgram{q}s & size\\
\hline
Sorted & 2 & 210 & \numprint{44100}\\
& 3 & \numprint{1540} & \numprint{2371600}\\
\hline
Unsorted & 2 & 400 & \numprint{160000}\\
& 3 & \numprint{8000} & \numprint{6400000}\\
\end{tabular}
\caption{Number of \Qgram{q}s, \(q\in\{2,3\}\), to be enumerated using 20
amino
acids and the resulted score matrix size}\label{tab:numdiff}
\end{center}
\end{table}
\SKcomment{I would not mention SIMD here, as this is an implementation
issue,
and the SIMD-based method does not improve the running time as far as I
remember.}
\AVTcomment{Removed. I would then assume that the SIMD benchmarks are also
to be
removed. Should I write a short discussion about this in discussion
section?}
\SKcomment{Yes. Put it into the discussion}

\section{Working with sorted \Qgram{q}s}
\subsubsection{Enumeration}
In mathematics, a set is defined as a collection of items, where every
element occurs exactly once. This definition can be expanded to multisets,
where elements can occur more than once. Applying an order on these
multisets, we can formalize a notion of sorted \Qgram{q}s:
Given an alphabet \(\Alpha\) and a natural number \(q > 0\), a sorted
\Qgram{q} over \(\Alpha\) is a multiset \(M=a_0a_1...a_{q-1}\),
\(a_i\in\Alpha\) for all \(i\), \(0\leq i<q\) and
\(a_i\leq a_{i+1}\) for all \(i\), \(0\leq i < q-1\).
%The number of possible sorted \Qgram{q}s for a given \(\mathcal(A)\) and
\(q\) can be computed as the binomial \(\binom{q+|\Alpha|-1}{q}\). The
proof can be shown with induction:
An index table of sorted \Qgram{q}s can be computed recursively.
Figure~\ref{fig:enumerate} showcases a model of the problem where \(q=5\)
and \(\Alpha=\{a_0,a_1,a_2\}\). If the prefix of length 2 of \(u\) is
fixed, say \(a_0a_1\), the last 3 characters can be iterated as a sorted
\Qgram{q} of size \(5-2=3\) and alphabet \(\Alpha_1 = \{a_1,a_2\}\). This
could be broken down further and generalized.
Given a sorted \Qgram{q} \(u\) where the prefix of \(m\) characters is
sorted and fixed, and \(u[m-1]\) has a rank of \(a_i\) then the task is to
enumerate every \Qgram{q-m}s of alphabet size (\(|\Alpha|-a_i\)).
The induction base case is then sorted unigrams over alphabet \(\Alpha_i =
\{a_j| a_j\in\Alpha \text{ and } j\geq i\}\), where there are \(|\Alpha|-i\)
unigrams indexed from \(a_i\) to \(|\Alpha|-1\).
\begin{figure}[t]
\begin{center}
\begin{tikzpicture}
[arraynode/.style={inner sep=1pt,minimum height=19pt,minimum
width=19pt,rectangle,draw},
alphanode/.style={inner sep=1pt,minimum height=15pt,minimum
width=15pt,rectangle},
position label/.style={
below = 3pt,
text height = 1.5ex,
text depth = 1ex
},
overbrace/.style={decoration={brace},decorate},
underbrace/.style={decoration={brace,mirror},decorate}
]
\begin{scope}[start chain=1 going right,node distance=-0.5mm]
\node [alphanode,on chain=1] {\(\Alpha:\)};
\node [alphanode,on chain=1] {\FirstMultisetSymbol};
\node [alphanode,on chain=1] (A0) {\SecondMultisetSymbol};
\node [alphanode,on chain=1] (A1) {\ThirdMultisetSymbol};
\draw [overbrace,decoration={raise=-1ex}] (A0.north west) --
node [position label,yshift=2.5ex] {\(\Alpha_1\)} (A1.north east);
\end{scope}
\begin{scope}[shift={(0cm,-17pt)},start chain=2 going right,node
distance=-0.15mm]
\node [arraynode,on chain=2] {\FirstMultisetSymbol};
\node [arraynode,on chain=2] (Posb) {\SecondMultisetSymbol};
\node [arraynode,on chain=2] (N0) {};
\node [arraynode,on chain=2] {};
\node [arraynode,on chain=2] (N1) {};
\draw [underbrace,decoration={raise=1pt}] (N0.south west) --
node [position label,yshift=-1ex] {\(m=3\)} (N1.south east);
\end{scope}
\end{tikzpicture}
\caption{Example of sorted \Qgram{q}
enumerating\label{fig:enumerate}~\cite{pfn}}
\end{center}
\end{figure}
\begin{algorithm}[t]
\caption{\(\mathsf{MultisetEnumerateRecursion}\)}
\label{code:enumerateMultisetRec}
\begin{tabular}{@{}l@{~}l}
\textbf{Input:}&current multiset length \(q_i\)\\
&current alphabet \(\Alpha_i\)\\
&current prefix \(u\)\\
&current index table \(\IT{q}{\Alpha}\)
\end{tabular}
\begin{algorithmic}
\If{\(q_i = 0\)}
\State \(\IT{q}{\Alpha}.\Append(u)\)
\Else
\For{\(a_j \in \Alpha_i\)}
\State \(\mathsf{MultisetEnumerateRecursion}(q_i-1,\Alpha_i \backslash
[a_i,a_j),u+a_j,\IT{q}{\Alpha}\)
\EndFor
\EndIf
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[t]
\caption{Enumerate Multiset \(\IT{q}{\Alpha}\)}
\label{code:enumerateMultiset}
\begin{tabular}{@{}l@{~}l}
\textbf{Input:}&multiset length \(q\)\\
&alphabet \(\Alpha\)
\end{tabular}
\begin{algorithmic}
\State \(\IT{q}{\Alpha}\gets \lbrack \rbrack\)
\For{\(a_i \in \Alpha\)}
\State \(\mathsf{MultisetEnumerateRecursion}(q-1,\Alpha \backslash
[a_0,a_i),a_i,\IT{q}{\Alpha})\)
\EndFor
\end{algorithmic}
\end{algorithm}
\subsubsection{Linear encoding}
Using the index table, a scheme to encode any given \Qgram{q} over
alphabet \(\Alpha\) to its table index in \(O(q)\) can be devised.
\begin{Lemma}
\label{linearEncodingProof}
There exists an unique integer weight for each character \(a\) in position
\(p\) so that given any sorted \Qgram{q} \(u\) over an alphabet
\(\Alpha\),
\begin{align}
c(u)=\sum_{p=0}^{q-1} w(p,u[p])\label{Equation:linearenc}
\end{align}
encodes exactly \(u\) to its index in table IT.
\begin{Proof}
\textbf{Base case:} In case of \(q=1\), \(|\Alpha|\) unigrams can be
clearly encoded with \(w(0,a) = a\) for all \(a \in \Alpha\).
\textbf{Inductive step:} Given the above scheme is valid up until
\(q-1\), \(q\in \Nats\), \(q\geq 2\), it needs to be shown that
\(w(q,a)\) is unique for
all \(a\in \Alpha\). Assuming the weight isn't unique, therefore there
exist two different weights \(w_a\neq w_a'\) so that \(c(au) = w_a + c(u)\)
and \(c(av) = w_a' + c(v)\) encode \Qgram{q}s \(au\) and \(av\)
respectively, \(u,v\in \Alpha^{q-1},a\in\Alpha\). The codes \(c(au)\) and
\(c(av)\) must but differ exactly the code of their suffixes \(c(u)-c(v)\),
since they share the same prefix. One can then formulate:
\begin{align}
c(au) - c(av) &= (w_a + c(u)) - (w_a' + c(v))\\
&= (w_a - w_a') + (c(u) - c(v))\\
&= c(u) - c(v)
\end{align}
which leads to \(w_a - w_a' = 0\), or \(w_a = w_a'\), which is a
contradiction. Therefore \(w(q,a)\) must be unique.
\end{Proof}
\end{Lemma}
The recursive method to compute a \(q\times |\Alpha|\) table LE is then
deduced based on the above proof. The base case can be directly given and
based on the index tables \(\mathsf{IT}_{q_i,\Alpha},2 \leq q_i \leq q\),
the \Qgram{q_i}s are tracked for change in prefix and the weight of the
prefix can be calculated with Equation~\ref{Equation:linearenc}. The
pseudocode for the construction of LE is outlined in
Algorithm~\ref{code:linearEncodingTable}.
\begin{algorithm}[t]
\caption{Creating linear encoding table \(\LE{q}{\Alpha}\)}
\label{code:linearEncodingTable}
\begin{tabular}{@{}l@{~}l}
\textbf{Input:}&sequence length \(q\)\\
&alphabet \(\Alpha\)\\
&index tables \(\text{IT}_{q_i,\Alpha}\text{ for all }q_i \in [1,q]\)\\
\end{tabular}
\begin{algorithmic}
\State \(\LE{q}{\Alpha}\) \(\gets []\)
\State \(\LE{q}{\Alpha}.\Append([0,...,|\Alpha|-1]\))
\For{\(q_i\in[2,q]\)}
\State \(\CurrentWeight \gets [None \times |\Alpha|]\)
\For{\(i,u\in\text{IT}_{q_i,\Alpha}\)}\Comment{Key-Value loop}
\If{\(\CurrentWeight [u[0]] = None\)}
\State \(\SuffixCode\gets 0\)
\For{\(j \in [1,q_i]\)}
\State \(\SuffixCode += \LE{q}{\Alpha}[j-1][u[j]]\)
\EndFor
\State \(\CurrentWeight [u[0]] \gets i - \SuffixCode\)
\EndIf
\EndFor
\State \(\LE{q}{\Alpha}.\mathsf{insert}(0,\CurrentWeight)\)\Comment{Place
more significant weight at beginning}
\EndFor
\end{algorithmic}
\end{algorithm}
\section{Simplified approach in determining threshold}
In MMseqs, in order to evaluate a proper threshold \(k\) for the
environment, a \(z\)-score statistics was applied. For each query sequence,
a calibration search through a subset of 100~000 randomly sampled target
sequences is performed, where the number of prefiltering operations
\begin{align}
\text{sum}_L = \sum_{t=1}^{100~000} (L_t-k+1)
\end{align}
and its sum of scores
\begin{align}
\text{sum}_S = \sum_{t=1}^{100~000} S_{qt}
\end{align}
are recorded. \(L_t\) here denotes the length of the target sequence \(t\)
and \(S_{qt}\) the prefiltering score between query sequence \(s\) and
target sequence \(t\). The expected chance prefiltering score between them
is then
\begin{align}
S_0 = (L_t-k+1)\frac{\text{sum}_S}{\text{sum}_L}
\end{align}
Assuming the number of \Qgram{q} matches is Poisson-distributed, the
standard deviation of the scores \(\sigma_S\) can be computed through
number of expected \Qgram{q} matches \(n_{\text{match}}\):
\begin{align}
n_{\text{match}} &\approx \frac{S_0}{S_{\text{match}}}
\end{align}
\begin{align}
\sigma_S &= S_{\text{match}}\sqrt{n_{\text{match}}}\\
&= \sqrt{(L_t-k+1)\frac{\text{sum}_S}{\text{sum}_L}S_{\text{match}}}
\end{align}
where \(S_{\text{match}}\) is the expected score per chance \Qgram{q}
match. The significant prefiltering score \(S_{qt}\) should then fulfill
the condition
\begin{align}
S_{qt} \geq Z_{thr}\sigma_S + S_0
\end{align}
where \(Z_{thr}\) is the significant \(z\)-score. MMseqs2 would take in a
sensitivity parameter \(s\), labeled internally as the average length of
\Qgram{q} list each sequence position, from the user and using a heuristic
to compute for \(S_{qt}\). This approach is shown to be lacking in control
for \(s\) and therefore the goal is to streamline the process and to allow
for a more fine-grained control of the length of \Qgram{q} list.

\textbf{A context-free approach:} The average \Qgram{q} list \(l\) and the
threshold \(k\) of the environment can be related through the right tail
portion of a discrete distribution of
unsorted-\Qgram{q}-vs-unsorted-\Qgram{q} score \(h_q\).
\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{graphics/distribution.png}
\end{center}
\caption{Example of score distribution \(h_7\) between uniformly
distributed \Qgram{7}. Blue denotes portion of the distribution where
\(\sigma < 0\). Red denotes portion of the distribution where \(\sigma \geq
0\).}
\label{fig:distrbution}
\end{figure}
Figure~\ref{fig:distrbution} visualizes an exemplary score distribution
between uniformly distributed \Qgram{7}. The red portion in the right
tail describes the probability \(\mathcal{P}(\sigma \geq 0)\), which could
be generally quantified to:
\begin{align}
\mathcal{P}(\sigma \geq k) = \sum_{i = k}^\infty h_q(i)
\end{align}
The average \Qgram{q} list length \(l\) could be normalized to
probability:
\begin{align}
p = \frac{l}{|\Alpha|^q}
\end{align}
Therefore the approximation would be ideally in continuous distribution an
exact value:
\begin{align}
p = \mathcal{P}(\sigma \geq k)
\end{align}
or in discrete distribution, a value between two discrete values:
\begin{align}
\mathcal{P}(\sigma \geq k) \leq p < \mathcal{P}(\sigma \geq
k-1)\label{eq:distribution}
\end{align}
The idea of the algorithm is to approximate a \(q\times q\) score
distribution. Then the distribution can be iterated from the right tail to
calculate \(\mathcal{P}(\sigma \geq k)\), where \(k\) is the current score
threshold. The iteration can be stopped when Equation~\ref{eq:distribution}
is satisfied.
The \(q\times q\) score distribution can be approximated recursively
through convolution:
\begin{align}
h_{q}(x) = \sum_{y\in\mathcal{Z}} h_{q-1}(y)h_1(x-y)
\label{equation:convolution}
\end{align}
It is important to note that \(h_1\), the 1-gram score distribution,
depends on both character distribution collected from target sequence and
uniform character distribution, since the characters appeared in the
environment are independent from each other.

\textbf{Integrating context:} The above strategy treats single character as
independent and therefore loses its context in \Qgram{q}s. Another
approach could then be hashing the target sequences to collect \Qgram{q}
distribution, which is convenient since the work is done in the
preprocessing of target dabase. This approach is but often complicated in
large \(q\), since a \(q\times q\) score matrix must be computed, which
incur an additional penalty of \(O(|\Alpha|^{2q})\), which is often
not feasible when \(q>4\). A middle ground is to divide the \Qgram{q}s
into two or three sub-\Qgram{q}s and collect their distribution
separately. These distributions can then in the end be convoluted together
to approximate the \Qgram{q} distribution. The extraction of threshold
\(k\) then follows the same approach in the context-free variant.
\Chapter{Implementation and Design}
\section{Design overview}
The program consists of seven main classes:
\begin{enumerate}
\item Seed readers convert seed into binary representation, computing its
span and weight. To further process seed, two variants are created:
\begin{enumerate}
\item Target reader divides seeds into blocks of shorter \Qgram{q}s as
input for recursive hashing function.
\item Query reader divides seeds into blocks of short segments, sorts query
sequences and uses sorted \Qgram{q} encoder to linearly encode the
resulted sorted \Qgram{q}s.
\end{enumerate}
\item A recursive hash engine to collect every \Qgram{q} hashes from
target sequences and package them.
\item A sorted \Qgram{q} index table to index every sorted \Qgram{q} of
given
scoring function and \(q\).
\item Sorted \Qgram{q} encoder creates a table to help linearly encode
sorted \Qgram{q}s.
\item A class to approximate an appropriate threshold given target sequence
data.
\item A score matrix to cache all scores between sorted \Qgram{q}s and
unsorted \Qgram{q}s.
\item An environment constructor gets sorted \Qgram{q} codes from the
query seed reader, uses them to call individual scores from the score
matrices and generates a Cartesian product from the environments.
\end{enumerate}
In Figure~\ref{fig:classes}, the relationship and dataflow between the
classes are outlined. Since the implementation of the recursive hashing of
target sequences is straightforward and doesn't diverge much from
pseudocode, the section below focuses mainly on the processing of query
sequences.
\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.5]{graphics/Class_Diagram.png}
\end{center}
\caption{Relationship between the main classes and inputs}
\label{fig:classes}
\end{figure}
\section{Input}
At compile time, the program takes in a scoring function, which in the
context of protein sequences could be a BLOSUM or PAM matrix, a spaced seed
and a recursive hashing function. The scoring function should detail the
relevant alphabet, the transformer function, which encodes every character
in the alphabet to its corresponding rank, and a matrix of scores between
every pairs of characters. Internally, the spaced seed is initially
represented as a numeric constant, which during computation will be
transcoded into a bitset. The span of the seed can then be computed
according to the position of the first 1 in the bitset and also the seed
weight can be calculated by counting 1s.
At run time the program takes in two sequence databases in FASTA format.
Any data error, for example empty file or wrong data format will
automatically lead to termination of the program and an error message will
be logged~\cite{gttl}. Additionally, the sensitivity of the program can be
adjusted as a command line argument.
\section{Compile time computation}
Figure~\ref{fig:classes} shows that some classes/tables can be evaluated at
compile time. Each seed gets transcoded and its weight and span can be
precomputed and therefore, the two seed readers and their schematics can be
predetermined. The two index tables for sorted/unsorted 2-grams/3-grams
take in only scoring function as parameter and therefore, can be wholely
precomputed. Since the linear encoding table LE (see
Algorithm~\ref{code:linearEncodingTable}) based solely on the sorted
2-grams/3-grams index table, it also can be evaluated at compilation.
\section{Run time computation}
At run time, firstly the target and query database gets encoded using the
transformer function. Using the target seed reader, the target sequences
can then be hashed in linear \(O(nb)\) time and the hashes are packaged as
byte units and saved in a contiguous container (i.e.\ array or vector).
In the processing of query sequences, firstly the \Qgram{2} and \Qgram{3}
score matrices \(\Scoretablename\) can be evaluated using the index tables
from sorted and unsorted \Qgram{q}s according to
Equation~\ref{equation:sequenceScore}. The result then can then be stored
as a pair of score and unsorted \Qgram{q} code and sorted after score
value for further use. Thereafter, an appropriate threshold is evaluated
using the target sequences and an user-provided sensitivity. Using the
score matrices and the threshold, the program enters the step of \Qgram{q}
generation using the Cartesian product.
\subsubsection{Local composition bias correction}
In the implementation of MMseqs2, a mechanism to evaluate the surrounding
of current position while iterating the query sequences is introduced. The
idea of this mechanism is to adjust the threshold of the \Qgram{q}
environment in response to regions where local composition varies
considerably from the background distribution. Without adjusting, these low
regions can lead to biases in the prefiltering result. This correction of
the threshold can be summarized as follow:

\begin{align}
\Delta \sigma_i(u[i]) =
\sum_{a=1}^{|\Alpha|}f(a)\sigma(a,u[i])-\frac{1}{2d}\sum_{j=i-d,j\neq
i}^{i+d}\sigma (u[i],u[j])
\end{align}

where \(u\) is the query sequence, \(i\) the current residue on the
sequence and \(f(a)\) the background frequency of residue \(a\).
The minuend is a representation of the expected score resulting from the
background distribution, which can be precomputed when the target data is
read. The subtrahend involves the current region in the query sequence and
introduces a parameter \(d\) for the radius of the region, defined in
MMseqs2 as a constant 20.
The corrected score between a query \Qgram{q} \(u\) and its generated
\Qgram{q} \(v\) can then be computed as the sum of the pairwise amino acid
score and the score correction.

\begin{align}
\sigma_c(u,v) = \sigma (u,v) + \sum_{i=0}^{q-1} \sigma_i(u[i])
\end{align}

In the implementation, this correction is instead subtracted from the
threshold at the beginning of the computation.

\begin{align}
\sigma_c(u,v) = \sigma (u,v) + \sum_{i=0}^{q-1} \sigma_i(u[i]) \geq k
\Leftrightarrow \sigma (u,v) \geq k - \sum_{i=0}^{q-1} \sigma_i(u[i]) =
k_{c}
\end{align}

\subsubsection{Turning Wheel Mechanism}
An issue in enumerating the Cartesian product is the possible difference in
number of loops (e.g.\ 2 loops for seed weight 4-6, 3 loops for seed weight
7). To resolve this problem and allow easy expansion to greater seed
weight, a flexible loop structure is designed, where an array containing
the loop indexes is created. The earliest entry of the array holds the
outermost loop index and the later an entry is, the more inner the loop
index the entry holds. By iterating through only the innermost loop and
only adjusting the outer loop indexes as needed, the scheme can account for
any number of loops.
The formulation of the loop structure, is outlined in the
pseudocode~\ref{code:turningWheel}.
\begin{algorithm}[t]
\caption{Turning Wheel Mechanism}
\label{code:turningWheel}
\begin{tabular}{@{}l@{~}l}
\textbf{Input:}&number of loops \(n\)\\
&loop data vector \(v_0\), \(v_1\), ..., \(v_{n-1}\)\\
&threshold \(k\)\\
&function \(\mathsf{createQgram}\)\\
&function \(\mathsf{getScore}\)\\
&function \(\mathsf{getCode}\)
\end{tabular}
\begin{algorithmic}
\State \(\Loopid \gets n-1\)
\State \(\Wheel \gets [0]\times n\)
\State \(\Score \gets \mathsf{getScore}(\Wheel)\)
\If{\(\Score < k\)} \Comment{Current data vectors not viable. Early
Termination.}
\State return
\Else \Comment{First loop iteration}
\State \(\Code \gets \mathsf{getCode}(\Wheel)\)
\State \(\mathsf{createQgram}(\Code)\)
\EndIf
\While{True}
\State \(\Wheel[\Loopid]\gets \Wheel[\Loopid]+1\)
\If{\(\Wheel[\Loopid] \geq \text{len}(v_{\Loopid})\)}\Comment{End of loop}
\State \(\Wheel[\Loopid] \gets 0\)
\If{\(\Loopid = 0\)}\Comment{No longer viable in outermost loop}
\State break
\EndIf
\State \(\Loopid\gets \Loopid-1\)
\Else
\State \(\Score \gets \mathsf{getScore}(\Wheel)\)
\If{\(\Score \geq k\)}\Comment{Viable \Qgram{q} found}
\State \(\Code \gets \mathsf{getCode}(\Wheel)\)
\State \(\mathsf{createQgram}(\Code)\)
\Else \Comment{No viable \Qgram{q} found, end current loop}
\State \(\Wheel[\Loopid]\gets \text{len}(v_{\Loopid})\)
\EndIf
\If{\(\Loopid \neq n-1\)}\Comment{Reset to innermost loop}
\State \(\Loopid \gets n-1\)
\EndIf
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}
\begin{comment}
\subsubsection{SIMD}
SIMD (Single-Instruction Multiple-Data) is a group of instructions allowing
vectorized, parallel data processing. Instead of processing data
sequentially, they are capable of defining wide registers, commonly 128-bit
or 256-bit, packaging data in those registers and process data in parallel.
The key to the shuffle using SIMD is the instruction
\(\mathsf{\_\_mm\_shuffle\_epi8}\), which is available on platforms
supporting
SSSE3 instruction sets or the equivalent \(\mathsf{vqtbl1q\_u8}\) on ARM
platform. Both instructions take in two 128-bit (equivalent to 16-byte)
vectors.
In order to comply with the instructions, both the \Qgram{q} and the
inverse
permutation get padded internally to 16-byte. First vector holds the data,
in
this case the \Qgram{q}, and the second the information on how the data get
shuffled, in this case the inverse permutation.
\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.6]{graphics/SIMD.png}
\end{center}
\caption{Schematics of SIMD instruction \_\_mm\_shuffle\_epi8}
\label{fig:simd}
\end{figure}
A schematic is visualized in Figure~\ref{fig:simd}. The right padded
section of the inverse permutation is filled with integer 128 to signal the
instruction to not shuffle and to simply pad the resulted vector with 0.
\end{comment}
\section{Merging target and query data}
After being collected, the hashed data from the target and query sequences
are packaged as byte units, prioritizing hash values. They then get sorted
individually using radix sort, resulting in two data vectors sorted by hash
values which would then be merged value by value, skipping through
unmatching
blocks.
\begin{algorithm}[t]
\caption{Merging query and target data}
\label{code:merging_data}
\begin{tabular}{@{}l@{~}l}
\textbf{Input:}&query data \(\Query\)\\
&target data \(\Target\)\\
&function \(\mathsf{CreateMatch}\)
\end{tabular}
\begin{algorithmic}
\State \(\TargetIdx \gets 0\)
\State \(\QueryIdx \gets 0\)
\While{\(\TargetIdx \neq \Target.\End\) \(\text{ and }\) \(\QueryIdx \neq
\Query.\End\)}
\State \(\HashValue_t \gets \Target[\TargetIdx].\HashValue\)
\State \(\HashValue_q \gets \Query[\QueryIdx].\HashValue\)
\If{\(\HashValue_t < \HashValue_q\)} \Comment{Skipping on target vector}
\While{\(\Target[\TargetIdx].\HashValue = \HashValue_t\)}
\State \(\TargetIdx\gets\TargetIdx+1\)
\EndWhile
\Else
\If{\(\HashValue_t > \HashValue_q\)} \Comment{Skipping on query vector}
\While{\(\Query[\QueryIdx].\HashValue = \HashValue_q\)}
\State \(\QueryIdx\gets\QueryIdx+1\)
\EndWhile
\Else \Comment{Match found}
\State \(\TBlockEnd = \TargetIdx\)
\While{\(\Target[\TBlockEnd] = \HashValue_t\)}
\State \(\TBlockEnd\gets\TBlockEnd+1\)
\EndWhile
\State \(\QBlockEnd = \QueryIdx\)
\While{\(\Query[\QBlockEnd] = \HashValue_q\)}
\State \(\QBlockEnd\gets\QBlockEnd+1\)
\EndWhile
\State
\(\mathsf{CreateMatch}([\TargetIdx,\TBlockEnd),[\QueryIdx,\QBlockEnd))\)
\State \(\TargetIdx\gets\TBlockEnd\)
\State \(\QueryIdx\gets\QBlockEnd\)
\EndIf
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}
The merging process results in a vector of matches, represented again as
byte units, containing respectively the sequence number of the match on the
target, on the query, the diagonal number (difference between the target
and query sequence position), and the query sequence position. These
quartets are again sorted with radix sort to prepare for ungapped alignment
stage.
\Chapter{Benchmark and Performance}
Separate modular benchmarks were carried out for the processing of target
sequences, threshold estimation and similar \Qgram{q} generation. All
measurements were performed using C++ on an Intel CPU i5-\numprint{10300}H
2.5~GHz with 8~GB DDR4 running Ubuntu on a SSD. The program was compiled
with
g++ 11.3.0.
The test dataset composed of a target database from MMseqs2 and a query
database created as a difference set from MMseqs2 queryset against the
target database. Some statistics on these datasets are summarized below in
Table~\ref{tab:datasets}. The scoring function used in testing is the
BLOSUM62
score matrix.
\section{Processing target sequences}
Testing from Figure~\ref{fig:target_hashing} showed that the direct
extraction of \Qgram{q}s generally has a better run time, as expected from
Table~\ref{tab:seeds}. Even the hashing using seed \numprint{111011}, which
was
predicted to have performance speedup, failed to show improvement. The
reason
could be repeated lookup of character to remove/add causing cache-miss,
slowing
the computation.
\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.45]{graphics/target_hashing.png}
\end{center}
\caption{Performance of two hashing functions}
\label{fig:target_hashing}
\end{figure}
\section{Determining threshold}
In order to test the accuracy of the threshold estimation, measurements
were firstly carried out on MMseqs2 with default seed.
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/mmseqs2_sensitivity.png}
\caption{Ratio of number of generated \Qgram{q}s using MMseqs2 approach
vs. expected}
\label{fig:mmseqssens}
\end{figure}
The test shows that MMseqs2 often overgenerated \Qgram{q}s, with the worst
case creating 231.11 times more than expected.
Both of the approaches, context-free and context-sensitive, were tested
with the
dividing scheme in context-sensitive approach reused from the \Qgram{q}
generation algorithm. For each test, the total number of \Qgram{q}s
generated is
averaged and then compared to the expected list length per position and
MMseqs2
result.
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/threshold_contextfree.png}
\caption{Ratio of number of generated \Qgram{q}s using context-free
approach vs. expected}
\label{fig:numctxfree}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/threshold_contextsens.png}
\caption{Ratio of number of generated \Qgram{q}s using context-free
approach vs. expected}
\label{fig:numctxsens}
\end{figure}
The benchmarks of context-free (Figure~\ref{fig:numctxfree}) and
context-sensitive approach (Figure~\ref{fig:numctxsens}) both show better
control compared to MMseqs2 and the resulted list lengths are relatively
close to the expected value. The thresholds created by this approach show
to be a good upper bounds for the \(k\)-environment in lower \(q\), where
only sensitivity of 1 in the seed weight 6 crosses the expected ammount. In
higher seed weight, both approaches serve well as an approximation, where
only a maximum of 15~\% more \Qgram{q}s were generated compared to
expected value.
It's also noticeable that both approaches result in the same
threshold, which could be due to the relatively short sub-\Qgram{q} length.
Since the dividing schema in the branch-and-bound algorithm is reused,
histograms
of \Qgram{2} and \Qgram{3} were created and due to their small length, they
are
not sufficient to diverge from the character distribution used in
context-free
approach. In order to discern the usage of each approach,the run time
of each method is investigated further in Figure~\ref{fig:sens_runtime}:
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/speed_threshold.png}
\caption{Run time comparison of context-free vs. context-sensitive
approach}
\label{fig:sens_runtime}
\end{figure}
The context-free approach shows to take very little time, almost
instantaneous.
Coupled with the very good estimation of the threshold it proves to be an
universal on-the-fly solution. The context-sensitive approach takes a lot
more
time, average around 0.5~s to create a threshold (compensated for creation
of
unsorted-unsorted \Qgram{q} score matrices, see
Section~\ref{section:scorematrix}),since it incurs a \(O(|\Alpha|^{2q})\)
time to build histogram. Therefore this approach should be used only on
high sensitivity, where long computation time is expected, or cached in the
preprocessing of target sequences.
It is important to note that since the approximation of the threshold \(k\)
doesn't take local score correction into account, all of the above
measurements are without local score correction. In
Figure~\ref{fig:compbias} are some benchmarks with correction of varying
degree, showcasing how the average list length is impacted. The result
shows that the average \Qgram{q} list can vary strongly and unpredictably
under usage of local composition bias score correction, where if the scale
of the correction is from 0.75~-~1, the resulted number can be as few as 5
times or as many as 48 times more than expected. The proper scaling of the
factor remains to be investigated further.
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/comp_bias.png}
\caption{Influence of local composition bias score correction on average
\Qgram{q} list length. The original estimation on \Qgram{q} list length
was 10.}
\label{fig:compbias}
\end{figure}
\section{Generating \Qgram{q}s}
In order to test the efficiency of the \Qgram{q} list generation, the
original workflow in MMseqs2 was replicated and tested against the new
algorithm. This workflow, along with both variants of the new algorithm,
with and without SIMD implementation, was measured in
Figure~\ref{fig:genw5}, \ref{fig:genw6} and \ref{fig:genw7} for different
seed weights.
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/gen_w5.png}
\caption{Time measured in miliseconds to generate \Qgram{q}s with
different sensitivities using seed \numprint{1101011}}
\label{fig:genw5}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/gen_w6.png}
\caption{Time measured in miliseconds to generate \Qgram{q}s with
different sensitivities using seed \numprint{11101101}}
\label{fig:genw6}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/gen_w7.png}
\caption{Time measured in miliseconds to generate \Qgram{q}s with
different sensitivities using seed \numprint{1111010101}}
\label{fig:genw7}
\end{figure}
The tests showed that the new algorithm has generally a 10~\% to 15~\%
performance loss compared to the MMseqs2 method. This is the overhead
caused by the sorting and reshuffled of \Qgram{q}. The tradeoff is the low
initial memory consumption, where only 340~MB of memory is consumed for the
score matrices instead of 1.80~GB (see Table~\ref{tab:memdiff}).
\begin{table}
\begin{center}
\begin{tabular}{c|c|}
\Qgram{q} type & Memory Allocated in Bytes\\
\hline
Using sorted \Qgram{q}s & 340704000\\
Using sorted \Qgram{q}s & 1811500000\\
\end{tabular}
\caption{Comparison of initial memory footprints in creation of score
matrices
between two approaches. Data collected using Intel
V-Tune.\label{tab:memdiff}}
\end{center}
\end{table}
On average, the
program spends 18.2~ns to generate a \Qgram{5}, 18.3~ns to generate a
\Qgram{6} and 20.1~ns to generate a \Qgram{7}. This shows that the
\Qgram{q}
generation scales generally well with \(q\), where an additional loop in
creation of \Qgram{7} causing an increase of 10~\% in run time. It could
then
be reasoned that the generation of \Qgram{8} and \Qgram{9} may follow the
same
trend, which coupled with the new approach in estimating threshold could
lead to
an expansion of seed weight (see Section~\ref{section:expand}).
\section{General Performance}
In this section, the overall run time of the whole program is profiled to
find hotspots and to figure the scalability of the system. The program will
be run with the most optimized settings, meaning context-free threshold
approximation and \Qgram{q} generation using sorted method. In Figure, the
recorded time in each step of the program is visualized.
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/program_w6.png}
\caption{Run time of different phases in the program, measured in
miliseconds,
using seed \numprint{11101101}, sensitivity 10, 100 and \numprint{1000}}
\label{fig:program_w6}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/program_w7.png}
\caption{Run time of different phases in the program, measured in
miliseconds,
using seed \numprint{1111010101}, sensitivity 10, 100 and \numprint{1000}}
\label{fig:program_w7}
\end{figure}
The test showed that the run time of processing target data is generally
negligible. The processing of match data could be more significant,
reaching
0.4~s in testing but still scales much better than \Qgram{q} generation
phase.
This means that the hotspot of the program lies in the \Qgram{q} generation
phase, specifically in the Cartesian product formation and in lesser
manner,
the radix sort of \Qgram{q} data. In the most heavy measurement, the
\Qgram{q}
generation phase still takes 1.5~-~2 times as long as the sorting after,
see
Figure~\ref{fig:hotspot_w6} and \ref{fig:hotspot_w7}.
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/hotspot_w6.png}
\caption{Run time of \Qgram{q} generation and radix sort, measured in
miliseconds,
using seed \numprint{11101101}, sensitivity 10, 100 and \numprint{1000}}
\label{fig:hotspot_w6}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/hotspot_w7.png}
\caption{Run time of \Qgram{q} generation and radix sort, measured in
miliseconds,
using seed \numprint{1111010101}, sensitivity 10, 100 and \numprint{1000}}
\label{fig:hotspot_w7}
\end{figure}
\Chapter{Discussion \& Future improvements}
\section{Performance}
The performance of the implementation using both sorted and unsorted
\Qgram{q}s
is compared to MMseqs2 in Figure~\ref{fig:mmseqs2comp}.
\begin{figure}
\centering
\includegraphics[scale=0.6]{graphics/mmseqs2comp.png}
\caption{Run time of the program using both sorted and unsorted \Qgram{q}s,
measured in ms, is compared to MMseqs2, in generation of 10, 100 and 1000
\Qgram{q} per sequence position}
\label{fig:mmseqs2comp}
\end{figure}
Compared to the MMseqs2 program, the implementation using unsorted
\Qgram{q}s
shows generally worse performance, with ... more time taken. On the other
hand,
the approach using sorted \Qgram{q}s presents better run time, especially
in
lower sensitivity. This shows that using optimization methods implemented
in
MMseqs2 could improve the run time even further.
Two further possible optimization methods are usage of SIMD intructions and
multithreadsing. Both of these methods were tried in the implementation
using
SIMD instruction \(\mathsf{\_\_mm\_shuffle\_epi8}\) and OpenMP library, but
unfortunately both showed little to no improvement on run time. Reasons for
these behaviors remains to be investigated.
\section{Features}
The new workflow was created with expandability in mind, therefore a lot of
the calculation allows for greater seed size and even greater sub-\Qgram{q}
size. A few of MMseqs2 features were also recreated, namely the ability to
scale local composition bias correction and sensitivity manually. The
design of the program also prioritizes flexibility, in that every
calculation option, ranging from MMseqs2 mode vs new sorted \Qgram{q}
method mode and both context-free and context-sensitive approach can be
changed on-the-fly without recompilation.
The program was intended to be a testing environment only, therefore custom
workflows in MMseqs2 were not recreated, namely the stepwise sensitive
search.
\section{Possibility of expanding seed\label{section:expand}}
Earlier it was shown that the run time of \Qgram{q} generation scales
almost constantly to seed, therefore some testing was created to test for
seeds of weight 8 and 9:
\AVTcomment{Waiting for BytesUnit fix}
\section{Compile-time calculation of
score matrices\label{section:scorematrix}}
Even though the class diagram in Figure~\ref{fig:classes} shows that it's
possible to compute the score matrix in compile time, since it only depends
on the two index tables of sorted/unsorted \Qgram{q}s, the implementation
decides to create the table in run time instead. The reason largely is due
to the high memory stack size requirements and lack of compile time sorting
algorithm. A manual compile time sorting implementation (i.e.\ quicksort)
would again demand more compile time instances. A separate benchmark in
Figure~\ref{fig:scorematrix} showed that this could incur a 0.6~s loss in
creating the sorted-unsorted \Qgram{3} score matrix or 3.3~s in creating
the unsorted-unsorted \Qgram{3} score matrix at the start of the
computation.
\begin{figure}
\centering
\includegraphics[scale=0.7]{graphics/scorematrix.png}
\caption{Time measured in miliseconds to generate \(q\times q\) score
matrix}
\label{fig:scorematrix}
\end{figure}
\Chapter{Conclusion}
In this paper, the prefiltering module of the MMseqs2 software suite was
examined and alternative optimization methods were tried and tested. A
possible improvement in processing target sequences through recursive
hashing, although has a better time complexity in theory, was shown to not
be a major improvement, other than in edge cases of low block numbers. A
new method of estimating threshold for better estimation of number of
generated \Qgram{q}s was implemented and showed to be a better estimation
approach, both in low and high sensitivity. Finally, an optimized
branch-and-bound algorithm was introduced and presented a lower memory
consumption in tradeoff for marginal increase in run time, allowing future
expansion into larger seed size.
%%%%<
%bibliography
\newpage
\small
% change the name of the bib-file to get your own bibliography
\bibliographystyle{unsrt}
\bibliography{template}
\normalsize

%%%%>
% appendix
\appendix
\noappendicestocpagenum
\addappheadtotoc
\Chapter{Some additional method descriptions}
\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c}
Dataset & Number of sequences & Maximum Sequence Length & Minimum Sequence
Length\\
\hline
QUERY\_DIFF & 426 & \numprint{4291} & 8\\
TARGET & \numprint{20000} & \numprint{8081} & 7\\
\end{tabular}
\caption{Datasets used in testing the modules\label{tab:datasets}}
\end{center}
\end{table}

\Chapter{Some Additional Data}
%%%%<
\Assertion
\end{document}
